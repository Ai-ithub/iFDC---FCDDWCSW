# -*- coding: utf-8 -*-
"""PreprocessDataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1K0-may3mVXPCWeEFaX0guk7jPf9FvwdQ
"""

from google.colab import drive
drive.mount('/content/mydrive')

import pandas as pd
import numpy as np

# Number of records
num_samples = 1_000_000

# Categories and ranges
damage_types = [
    "Clay & Iron", "Drilling Damage", "Fluid Loss", "Fluid Incompatibility", "Emulsion",
    "Rock/Fluid Interaction", "Completion Issue", "Corrosion Cracking",
    "Filtration Problem", "Ultra-Clean Fluid"
]
formations = ["Carbonate", "Sandstone", "Shale", "Dolomite", "Mixed"]
fluid_types = ["Brine", "Acid", "Mud", "Water-Based", "Oil-Based"]
completion_types = ["Open Hole", "Cased Hole", "Perforated", "Liner"]

# Numerical ranges
temperature_range = (50, 200)  # Celsius
pressure_range = (1000, 15000)  # psi
ph_range = (3.5, 9.0)
salinity_range = (10_000, 250_000)  # ppm
flow_rate_range = (10, 1500)  # bbl/day
permeability_range = (0.01, 500)  # mD
porosity_range = (5, 35)  # %

# Generate synthetic data
np.random.seed(42)
data = {
    "Well_ID": [f"WELL_{i:06}" for i in range(num_samples)],
    "Formation": np.random.choice(formations, num_samples),
    "Fluid_Type": np.random.choice(fluid_types, num_samples),
    "Completion_Type": np.random.choice(completion_types, num_samples),
    "Temperature_C": np.random.uniform(*temperature_range, num_samples),
    "Pressure_psi": np.random.uniform(*pressure_range, num_samples),
    "pH": np.random.uniform(*ph_range, num_samples),
    "Salinity_ppm": np.random.uniform(*salinity_range, num_samples),
    "Flow_Rate_bbl_day": np.random.uniform(*flow_rate_range, num_samples),
    "Permeability_mD": np.random.uniform(*permeability_range, num_samples),
    "Porosity_pct": np.random.uniform(*porosity_range, num_samples),
    "Damage_Type": np.random.choice(damage_types, num_samples)
}

# Create and save DataFrame
df = pd.DataFrame(data)
df.to_csv("formation_damage_dataset.csv", index=False)

print("✅ Dataset generated and saved as 'formation_damage_dataset.csv'")

import pandas as pd
import re

def clean_text_data(text):
    """
    تابع برای یکنواخت‌سازی داده‌های متنی:
    1. حذف فاصله‌های اضافی در ابتدا و انتها
    2. تبدیل تمام متن به حروف کوچک
    3. حذف فاصله‌های اضافی بین کلمات
    4. جایگزینی زیرخط‌ها با فاصله
    """
    if pd.isna(text):
        return text

    text = str(text).strip()  # حذف فاصله‌های اضافی ابتدا و انتها
    text = text.lower()  # تبدیل به حروف کوچک
    text = re.sub(r'\s+', ' ', text)  # حذف فاصله‌های اضافی بین کلمات
    text = text.replace('_', ' ')  # جایگزینی زیرخط با فاصله
    return text

# خواندن دیتافریم (فرض می‌کنیم df نام دیتافریم شماست)
# df = pd.read_csv('your_file.csv')
df = pd.DataFrame(data)
# لیست ستون‌های متنی که باید یکنواخت شوند
text_columns = ['Damage_Type', 'Formation', 'Fluid_Type', 'Completion_Type']

# اعمال تابع پاکسازی روی هر ستون
for col in text_columns:
    if col in df.columns:
        df[col] = df[col].apply(clean_text_data)
        # یا با استفاده از map:
        # df[col] = df[col].map(clean_text_data, na_action='ignore')

# نمایش نتایج
print(df[text_columns].head())

import pandas as pd
import re
from collections import defaultdict

# 1. خواندن دیتافریم
df = pd.DataFrame(data) # جایگزین با مسیر فایل شما

# 2. تعریف ستون‌های متنی برای پردازش
text_columns = ['Damage_Type', 'Formation', 'Fluid_Type', 'Completion_Type']

# 3. ایجاد دیکشنری برای تصحیح غلط‌های املایی
correction_dict = {
    'Damage_Type': {
        'corosion': 'corrosion',
        'mechnical': 'mechanical',
        'erosoin': 'erosion'
    },
    'Formation': {
        'asmary': 'asmari',
        'bangestan': 'bangestan',
        'khami': 'khami'
    },
    'Fluid_Type': {
        'crude': 'crude oil',
        'condensate': 'gas condensate',
        'water': 'water'
    },
    'Completion_Type': {
        'open hole': 'openhole',
        'cased hole': 'casedhole',
        'perforated': 'perforated'
    }
}

def clean_and_normalize(text, column_name):
    """
    تابع برای یکنواخت‌سازی و تصحیح داده‌های متنی
    """
    if pd.isna(text):
        return text

    # تبدیل به رشته و حذف فاصله‌های اضافی
    text = str(text).strip().lower()
    text = re.sub(r'\s+', ' ', text)

    # تصحیح غلط‌های املایی
    if column_name in correction_dict:
        for typo, correct in correction_dict[column_name].items():
            if re.search(r'\b' + re.escape(typo) + r'\b', text):
                text = text.replace(typo, correct)

    return text

# 4. اعمال یکنواخت‌سازی روی ستون‌ها
for col in text_columns:
    if col in df.columns:
        df[col] = df[col].apply(lambda x: clean_and_normalize(x, col))

# 5. استخراج مقادیر یکتا از هر ستون
unique_values = {}
for col in text_columns:
    if col in df.columns:
        unique_values[col] = df[col].dropna().unique().tolist()

# 6. ذخیره مقادیر یکتا در فایل‌های مرجع
for col, values in unique_values.items():
    with open(f'unique_values_{col}.txt', 'w', encoding='utf-8') as f:
        f.write(f"Unique values for {col} (Count: {len(values)}):\n")
        f.write("\n".join(sorted(values)))
        f.write("\n")

# 7. ایجاد گزارش کیفیت داده
quality_report = []
for col in text_columns:
    if col in df.columns:
        null_count = df[col].isnull().sum()
        unique_count = len(df[col].dropna().unique())
        quality_report.append({
            'Column': col,
            'Total_Values': len(df),
            'Null_Values': null_count,
            'Null_Percentage': round((null_count/len(df))*100, 2),
            'Unique_Values': unique_count
        })

# 8. ذخیره گزارش کیفیت داده
quality_df = pd.DataFrame(quality_report)
quality_df.to_csv('data_quality_report.csv', index=False)

# 9. نمایش نتایج
print("پردازش با موفقیت انجام شد. نتایج:")
print("\nنمونه‌ای از داده‌های پردازش شده:")
print(df[text_columns].head())

print("\nگزارش کیفیت داده:")
print(quality_df)

# 1. ذخیره دیتافریم اصلی با تغییرات اعمال شده
output_file = 'processed_data.csv'
df.to_csv(output_file, index=False, encoding='utf-8')

# 2. ذخیره نسخه اکسل (اختیاری)
try:
    output_excel = 'processed_data.xlsx'
    df.to_excel(output_excel, index=False)
    print(f"دیتافریم در فایل اکسل {output_excel} نیز ذخیره شد")
except Exception as e:
    print(f"ذخیره فایل اکسل با خطا مواجه شد: {str(e)}")

# 3. ایجاد یک دیتافریم جداگانه برای مقادیر یکتا (اختیاری)
unique_values_df = pd.DataFrame.from_dict(unique_values, orient='index').transpose()
unique_values_file = 'unique_values_summary.csv'
unique_values_df.to_csv(unique_values_file, index=False, encoding='utf-8')

# 4. نمایش پیام پایانی
print("\n" + "="*50)
print(f"دیتافریم پردازش شده با موفقیت در {output_file} ذخیره شد")
print(f"تعداد سطرهای پردازش شده: {len(df)}")
print(f"تعداد ستون‌های پردازش شده: {len(df.columns)}")
print("="*50)

# 5. نمایش نمونه‌ای از داده‌های ذخیره شده (اختیاری)
print("\nنمونه‌ای از داده‌های ذخیره شده:")
print(pd.read_csv(output_file)[text_columns].head(3).to_markdown())