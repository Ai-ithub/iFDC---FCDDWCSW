# -*- coding: utf-8 -*-
"""پیش‌پردازش داده‌های حفاری برای آموزش مدل.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZmRz0hcfTQ63QNm2wIcQ8FlRWWLP4Fqw
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

import os
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder
import numpy as np

# مسیر داده خام و مسیرهای خروجی
RAW_DATA_PATH = '/content/drive/MyDrive/24may_amin_dataset'
PROCESSED_PATH = os.path.join('/content/cleaned_dataset.parquet')
OUTLIERS_PATH = os.path.join('/content/outliers/outliers.parquet')

# بارگذاری فایل CSV یا Parquet
def load_raw_data(path):
    files = [f for f in os.listdir(path) if f.endswith(('.csv', '.parquet'))]
    if not files:
        print("🚨 هیچ فایل داده خام یافت نشد.")
        return None
    file = files[0]
    print(f"📂 بارگذاری فایل: {file}")
    file_path = os.path.join(path, file)
    if file.endswith('.csv'):
        return pd.read_csv(file_path)
    else:
        return pd.read_parquet(file_path)

# پاک‌سازی داده‌ها
def preprocess_data(df):
    print("🔍 پاک‌سازی مقادیر گمشده...")
    df = df.dropna()  # یا می‌تونی از imputing استفاده کنی

    print("🔬 نرمال‌سازی ویژگی‌های عددی...")
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    scaler = StandardScaler()
    if numeric_cols:
        df[numeric_cols] = scaler.fit_transform(df[numeric_cols])

    print("🔤 Encoding ویژگی‌های متنی...")
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    for col in categorical_cols:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col].astype(str))

    return df, numeric_cols

# شناسایی داده‌های پرت
def detect_outliers(df, numeric_cols):
    print("📊 شناسایی داده‌های پرت...")
    outlier_indices = set()
    for col in numeric_cols:
        col_data = df[col]
        # IQR Method
        Q1 = col_data.quantile(0.25)
        Q3 = col_data.quantile(0.75)
        IQR = Q3 - Q1
        outlier_step = 1.5 * IQR
        iqr_outliers = df[(col_data < Q1 - outlier_step) | (col_data > Q3 + outlier_step)].index
        outlier_indices.update(iqr_outliers)

        # Z-Score Method
        z_scores = (col_data - col_data.mean()) / col_data.std()
        z_outliers = df[np.abs(z_scores) > 3].index
        outlier_indices.update(z_outliers)

    outliers_df = df.loc[list(outlier_indices)]
    cleaned_df = df.drop(index=outlier_indices)

    return cleaned_df, outliers_df

# ذخیره‌سازی امن
def save_dataframe(df, path, original_columns):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    if df.empty:
        print(f"⚠️ DataFrame خالی است. ذخیره‌سازی ساختار ستون‌ها در {path}")
        pd.DataFrame(columns=original_columns).to_parquet(path, index=False)
    else:
        df.to_parquet(path, index=False)
        print(f"✅ داده ذخیره شد: {path}")

# اجرای Pipeline
def run_pipeline():
    df = load_raw_data(RAW_DATA_PATH)
    if df is None or df.empty:
        print("🚨 داده خام موجود نیست یا خالی است.")
        return

    original_columns = df.columns.tolist()
    processed_df, numeric_cols = preprocess_data(df)
    cleaned_df, outliers_df = detect_outliers(processed_df, numeric_cols)

    save_dataframe(cleaned_df, PROCESSED_PATH, original_columns)
    save_dataframe(outliers_df, OUTLIERS_PATH, original_columns)

    print("🎉 پیش‌پردازش کامل شد.")

if __name__ == "__main__":
    run_pipeline()

from google.colab import drive
import shutil

# Mount Google Drive
drive.mount('/content/drive')

# مسیر ورودی و خروجی
input_path = '/content/datasets'
output_zip_path = '/content/datasets.zip'
destination_path = '/content/drive/MyDrive/datasets.zip'

# فشرده‌سازی
shutil.make_archive(output_zip_path.replace('.zip', ''), 'zip', input_path)
print(f"✅ فایل زیپ ساخته شد: {output_zip_path}")

# انتقال به Google Drive
shutil.move(output_zip_path, destination_path)
print(f"✅ فایل زیپ منتقل شد: {destination_path}")